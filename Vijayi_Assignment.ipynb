{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13217100-8bd0-421e-97da-53289efc0987",
   "metadata": {},
   "source": [
    "# Task-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e3ec05a-d70e-4dd2-af54-efe1a8d28e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 3 tasks from file 'C:/Users/asus/Desktop/Text_Extractor/Extract.txt':\n",
      "\n",
      "{'task': 'hn must complete the annual budget review by next Friday', 'who': 'hn', 'deadline': 'Friday', 'category': 'Work'}\n",
      "{'task': 'Mary has to schedule the project kickoff meeting for tomorrow afternoon', 'who': 'Mary', 'deadline': 'tomorrow', 'category': 'Appointments'}\n",
      "{'task': 'Alex is required to finalize the vendor contract negotiations before the client event next Monday', 'who': 'Alex', 'deadline': 'Monday', 'category': 'General'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import dateparser\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Load the spaCy language model for English (small model)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a set of task-related keywords\n",
    "TASK_KEYWORDS = {\"must\", \"should\", \"has to\", \"have to\", \"needs to\", \"is required to\"}\n",
    "\n",
    "# Date Parsing Module\n",
    "\n",
    "def parse_date(date_text):\n",
    "    \"\"\"\n",
    "    Parses date text and returns a formatted date string.\n",
    "    \n",
    "    This function handles:\n",
    "      - Relative terms like \"today\" and \"tomorrow\".\n",
    "      - Weekday names (e.g., \"next Friday\") by computing the next occurrence.\n",
    "      - Other relative date expressions using dateparser.\n",
    "    \n",
    "    Parameters:\n",
    "        date_text (str): The date string extracted from text.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: A formatted date string (or day name) if parsing is successful;\n",
    "                     otherwise, None.\n",
    "    \"\"\"\n",
    "    # Check for simple relative dates\n",
    "    if \"today\" in date_text.lower():\n",
    "        return \"today\"\n",
    "    elif \"tomorrow\" in date_text.lower():\n",
    "        return \"tomorrow\"\n",
    "    \n",
    "    # Mapping for week days\n",
    "    week_days = {\n",
    "        \"monday\": 0, \"tuesday\": 1, \"wednesday\": 2, \"thursday\": 3,\n",
    "        \"friday\": 4, \"saturday\": 5, \"sunday\": 6\n",
    "    }\n",
    "    \n",
    "    # Look for any weekday mention in the date text\n",
    "    for day in week_days:\n",
    "        if day in date_text.lower():\n",
    "            today = datetime.today()\n",
    "            target_day = week_days[day]\n",
    "            days_ahead = target_day - today.weekday()\n",
    "            if days_ahead <= 0:  # If the day is today or in the past, select next week's occurrence\n",
    "                days_ahead += 7\n",
    "            next_weekday = today + timedelta(days=days_ahead)\n",
    "            return next_weekday.strftime('%A')  # Return the day name (e.g., 'Monday')\n",
    "    \n",
    "    # For other relative dates, use dateparser with settings favoring future dates\n",
    "    parsed_date = dateparser.parse(date_text, settings={'PREFER_DATES_FROM': 'future', 'RELATIVE_BASE': datetime.today()})\n",
    "    if parsed_date:\n",
    "        return parsed_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return None\n",
    "\n",
    "# Task Extraction Module\n",
    "\n",
    "def extract_task_details(sentences):\n",
    "    \"\"\"\n",
    "    Extracts task details (responsible entity, deadline, and category) from a list of sentences.\n",
    "    \n",
    "    The function processes each sentence to determine if it represents a task based on the presence\n",
    "    of predefined keywords. It then uses spaCy's named entity recognition to extract:\n",
    "      - 'who' is responsible for the task (e.g., a person or organization).\n",
    "      - The deadline by searching for date/time entities.\n",
    "      - The task category via keyword matching.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): List of sentences from the text.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: Each dictionary contains details for a task:\n",
    "                      {\n",
    "                          \"task\": original task sentence,\n",
    "                          \"who\": responsible person or organization,\n",
    "                          \"deadline\": extracted deadline or \"no deadline\",\n",
    "                          \"category\": assigned category of the task\n",
    "                      }\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Skip empty sentences after stripping whitespace\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "        \n",
    "        # Process sentence with spaCy NLP model\n",
    "        doc = nlp(sentence)\n",
    "        task_info = {\n",
    "            \"task\": sentence.strip(),\n",
    "            \"who\": None,\n",
    "            \"deadline\": \"no deadline\",\n",
    "            \"category\": None\n",
    "        }\n",
    "        \n",
    "        # Determine if the sentence likely represents a task using task keywords\n",
    "        if any(keyword in sentence.lower() for keyword in TASK_KEYWORDS):\n",
    "            # Attempt to extract the responsible entity (e.g., a PERSON or ORG)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
    "                    task_info[\"who\"] = ent.text\n",
    "                    break\n",
    "            \n",
    "            # If no entity is detected, try to extract the subject manually using dependency parsing\n",
    "            if not task_info[\"who\"]:\n",
    "                for token in doc:\n",
    "                    if token.dep_ == \"nsubj\":\n",
    "                        task_info[\"who\"] = token.text\n",
    "                        break\n",
    "            \n",
    "            # Extract deadline information from DATE or TIME entities\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in {\"DATE\", \"TIME\"}:\n",
    "                    parsed_date = parse_date(ent.text)\n",
    "                    if parsed_date:\n",
    "                        task_info[\"deadline\"] = parsed_date\n",
    "                    break  # Use the first encountered date/time entity\n",
    "            \n",
    "            # Categorize the task based on action words present in the sentence\n",
    "            task_info[\"category\"] = categorize_task(task_info[\"task\"])\n",
    "            \n",
    "            # Append the task details to the results list\n",
    "            tasks.append(task_info)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "\n",
    "# Task Categorization Module\n",
    "\n",
    "def categorize_task(task):\n",
    "    \"\"\"\n",
    "    Assigns a category to the task based on action keywords found in the task description.\n",
    "    \n",
    "    Supported categories include:\n",
    "      - Shopping, Cleaning, Work, Appointments.\n",
    "    \n",
    "    If no specific keywords are found, the task is assigned to the \"General\" category.\n",
    "    \n",
    "    Parameters:\n",
    "        task (str): The task description.\n",
    "    \n",
    "    Returns:\n",
    "        str: The category name.\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"Shopping\": [\"buy\", \"purchase\", \"snack\"],\n",
    "        \"Cleaning\": [\"clean\", \"sweep\", \"wash\", \"cleaning\"],\n",
    "        \"Work\": [\"submit\", \"review\", \"write\", \"send\", \"report\"],\n",
    "        \"Appointments\": [\"meet\", \"call\", \"schedule\", \"appointment\"]\n",
    "    }\n",
    "    \n",
    "    # Loop over each category and check if any keyword is present in the task\n",
    "    for category, keywords in categories.items():\n",
    "        if any(word in task.lower() for word in keywords):\n",
    "            return category\n",
    "    return \"General\"\n",
    "\n",
    "\n",
    "# File Reading and Processing Module\n",
    "\n",
    "def read_file_and_process(file_path):\n",
    "    \"\"\"\n",
    "    Reads text data from a file, splits it into sentences, and extracts task details.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input text file.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: A list of task details dictionaries extracted from the text.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Rudimentary sentence splitting (could be replaced with a more sophisticated method)\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # Extract task details from the list of sentences\n",
    "    tasks = extract_task_details(sentences)\n",
    "    return tasks\n",
    "\n",
    "# Main Pipeline Runner\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the task extraction pipeline.\n",
    "    \n",
    "    The function performs the following steps:\n",
    "      1. Checks for the existence of an input file at the specified path.\n",
    "      2. If found, processes the file to extract task details.\n",
    "      3. If not found, uses a manually curated sample text.\n",
    "      4. Outputs the extracted tasks.\n",
    "      5. Displays insights and challenges encountered during development.\n",
    "    \"\"\"\n",
    "    # Define the file path (update this path as needed)\n",
    "    file_path = \"C:/Users/asus/Desktop/Text_Extractor/Extract.txt\"\n",
    "    \n",
    "    # Check if the file exists; if not, use a manually curated sample\n",
    "    if os.path.exists(file_path):\n",
    "        tasks = read_file_and_process(file_path)\n",
    "        print(f\"Extracted {len(tasks)} tasks from file '{file_path}':\\n\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}. Using a manual sample instead.\\n\")\n",
    "        sample_text = (\n",
    "            \"John must submit the report by next Friday. \"\n",
    "            \"Mary should schedule an appointment for tomorrow. \"\n",
    "            \"Alex has to buy groceries today. \"\n",
    "            \"The team needs to review the project proposal next Monday.\"\n",
    "        )\n",
    "        # Split the sample text into sentences\n",
    "        sentences = sample_text.split('.')\n",
    "        tasks = extract_task_details(sentences)\n",
    "    \n",
    "    # Output the extracted tasks\n",
    "    for task in tasks:\n",
    "        print(task)\n",
    "   \n",
    "    \n",
    "\n",
    "# Execute the main pipeline if this script is run directly\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d7036fb-f895-43a5-9fbc-f36e160dc06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 3 tasks from file 'C:/Users/asus/Desktop/Text_Extractor/Extract.txt':\n",
      "\n",
      "{'task': 'john must complete the annual budget review by next Friday', 'who': 'john', 'deadline': 'next Friday', 'category': 'Work'}\n",
      "{'task': 'Mary has to schedule the project kickoff meeting for tomorrow afternoon', 'who': 'Mary', 'deadline': 'tomorrow', 'category': 'Appointments'}\n",
      "{'task': 'Alex is required to finalize the vendor contract negotiations before the client event next Weekend', 'who': 'Alex', 'deadline': 'next weekend', 'category': 'General'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import dateparser\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Load the spaCy language model for English (small model)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a set of task-related keywords\n",
    "TASK_KEYWORDS = {\"must\", \"should\", \"has to\", \"have to\", \"needs to\", \"is required to\"}\n",
    "\n",
    "# Date Parsing Module\n",
    "def parse_date(date_text):\n",
    "    \"\"\"\n",
    "    Parses date text and returns a textual representation of the date.\n",
    "    \n",
    "    Instead of computing the actual date, this function returns a text string\n",
    "    that reflects the relative date mentioned (e.g., \"today\", \"tomorrow\", \"next week\", \"next weekend\", or specific weekday names).\n",
    "    \n",
    "    Parameters:\n",
    "        date_text (str): The date string extracted from text.\n",
    "    \n",
    "    Returns:\n",
    "        str: A textual representation of the date if parsing is successful;\n",
    "             otherwise, returns the original text stripped.\n",
    "    \"\"\"\n",
    "    lower_text = date_text.lower().strip()\n",
    "    \n",
    "    # Check for simple relative dates\n",
    "    if \"today\" in lower_text:\n",
    "        return \"today\"\n",
    "    elif \"tomorrow\" in lower_text:\n",
    "        return \"tomorrow\"\n",
    "    \n",
    "    # Check for phrases with relative indicators in proper order:\n",
    "    if \"next weekend\" in lower_text:\n",
    "        return \"next weekend\"\n",
    "    if \"next week\" in lower_text:\n",
    "        return \"next week\"\n",
    "    if \"weekend\" in lower_text:\n",
    "        return \"weekend\"\n",
    "    \n",
    "    # Check for weekday names\n",
    "    week_days = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]\n",
    "    for day in week_days:\n",
    "        if day in lower_text:\n",
    "            if \"next\" in lower_text:\n",
    "                return \"next \" + day.capitalize()\n",
    "            else:\n",
    "                return day.capitalize()\n",
    "    \n",
    "    # For other relative dates, simply return the original text\n",
    "    return date_text.strip()\n",
    "\n",
    "# Task Extraction Module\n",
    "def extract_task_details(sentences):\n",
    "    \"\"\"\n",
    "    Extracts task details (responsible entity, deadline, and category) from a list of sentences.\n",
    "    \n",
    "    The function processes each sentence to determine if it represents a task based on the presence\n",
    "    of predefined keywords. It then uses spaCy's named entity recognition to extract:\n",
    "      - 'who' is responsible for the task (e.g., a person or organization).\n",
    "      - The deadline by searching for date/time entities.\n",
    "      - The task category via keyword matching.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list of str): List of sentences from the text.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: Each dictionary contains details for a task:\n",
    "                      {\n",
    "                          \"task\": original task sentence,\n",
    "                          \"who\": responsible person or organization,\n",
    "                          \"deadline\": extracted deadline or \"no deadline\",\n",
    "                          \"category\": assigned category of the task\n",
    "                      }\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Skip empty sentences after stripping whitespace\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "        \n",
    "        # Process sentence with spaCy NLP model\n",
    "        doc = nlp(sentence)\n",
    "        task_info = {\n",
    "            \"task\": sentence.strip(),\n",
    "            \"who\": None,\n",
    "            \"deadline\": \"no deadline\",\n",
    "            \"category\": None\n",
    "        }\n",
    "        \n",
    "        # Determine if the sentence likely represents a task using task keywords\n",
    "        if any(keyword in sentence.lower() for keyword in TASK_KEYWORDS):\n",
    "            # Attempt to extract the responsible entity (e.g., a PERSON or ORG)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
    "                    task_info[\"who\"] = ent.text\n",
    "                    break\n",
    "            \n",
    "            # If no entity is detected, try to extract the subject manually using dependency parsing\n",
    "            if not task_info[\"who\"]:\n",
    "                for token in doc:\n",
    "                    if token.dep_ == \"nsubj\":\n",
    "                        task_info[\"who\"] = token.text\n",
    "                        break\n",
    "            \n",
    "            # Extract deadline information from DATE or TIME entities\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in {\"DATE\", \"TIME\"}:\n",
    "                    parsed_date = parse_date(ent.text)\n",
    "                    if parsed_date:\n",
    "                        task_info[\"deadline\"] = parsed_date\n",
    "                    break  # Use the first encountered date/time entity\n",
    "            \n",
    "            # Categorize the task based on action words present in the sentence\n",
    "            task_info[\"category\"] = categorize_task(task_info[\"task\"])\n",
    "            \n",
    "            # Append the task details to the results list\n",
    "            tasks.append(task_info)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "# Task Categorization Module\n",
    "def categorize_task(task):\n",
    "    \"\"\"\n",
    "    Assigns a category to the task based on action keywords found in the task description.\n",
    "    \n",
    "    Supported categories include:\n",
    "      - Shopping, Cleaning, Work, Appointments.\n",
    "    \n",
    "    If no specific keywords are found, the task is assigned to the \"General\" category.\n",
    "    \n",
    "    Parameters:\n",
    "        task (str): The task description.\n",
    "    \n",
    "    Returns:\n",
    "        str: The category name.\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"Shopping\": [\"buy\", \"purchase\", \"snack\"],\n",
    "        \"Cleaning\": [\"clean\", \"sweep\", \"wash\", \"cleaning\"],\n",
    "        \"Work\": [\"submit\", \"review\", \"write\", \"send\", \"report\"],\n",
    "        \"Appointments\": [\"meet\", \"call\", \"schedule\", \"appointment\"]\n",
    "    }\n",
    "    \n",
    "    # Loop over each category and check if any keyword is present in the task\n",
    "    for category, keywords in categories.items():\n",
    "        if any(word in task.lower() for word in keywords):\n",
    "            return category\n",
    "    return \"General\"\n",
    "\n",
    "# File Reading and Processing Module\n",
    "def read_file_and_process(file_path):\n",
    "    \"\"\"\n",
    "    Reads text data from a file, splits it into sentences, and extracts task details.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input text file.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: A list of task details dictionaries extracted from the text.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Rudimentary sentence splitting (could be replaced with a more sophisticated method)\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # Extract task details from the list of sentences\n",
    "    tasks = extract_task_details(sentences)\n",
    "    return tasks\n",
    "\n",
    "# Main Pipeline Runner\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the task extraction pipeline.\n",
    "    \n",
    "    The function performs the following steps:\n",
    "      1. Checks for the existence of an input file at the specified path.\n",
    "      2. If found, processes the file to extract task details.\n",
    "      3. If not found, uses a manually curated sample text.\n",
    "      4. Outputs the extracted tasks.\n",
    "      5. Displays insights and challenges encountered during development.\n",
    "    \"\"\"\n",
    "    # Define the file path (update this path as needed)\n",
    "    file_path = \"C:/Users/asus/Desktop/Text_Extractor/Extract.txt\"\n",
    "    \n",
    "    # Check if the file exists; if not, use a manually curated sample\n",
    "    if os.path.exists(file_path):\n",
    "        tasks = read_file_and_process(file_path)\n",
    "        print(f\"Extracted {len(tasks)} tasks from file '{file_path}':\\n\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}. Using a manual sample instead.\\n\")\n",
    "        sample_text = (\n",
    "            \"Alex is required to finalize the vendor contract negotiations before the client event next Weekend. \"\n",
    "            \"John must submit the report by next Friday. \"\n",
    "            \"Mary should schedule an appointment for tomorrow.\"\n",
    "        )\n",
    "        # Split the sample text into sentences\n",
    "        sentences = sample_text.split('.')\n",
    "        tasks = extract_task_details(sentences)\n",
    "    \n",
    "    # Output the extracted tasks\n",
    "    for task in tasks:\n",
    "        print(task)\n",
    "   \n",
    "# Execute the main pipeline if this script is run directly\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e8974-9481-4e03-a347-f08db4028c0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Insights and Challenges\n",
    "    \n",
    "    \n",
    "    1. Handling varied date formats and relative date expressions required careful design.\")\n",
    "    2. Extracting the responsible entity ('who') is challenging when sentences lack clear named entities.\")\n",
    "    3. Basic sentence splitting by periods may fail for more complex text; using advanced segmentation could improve accuracy.\")\n",
    "    4. Categorizing tasks using simple keyword matching is effective for basic scenarios but may miss nuanced or multi-faceted tasks.\")\n",
    "    5. spaCy's language model might occasionally miss context-specific cues, affecting both entity recognition and dependency parsing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e33076-dafb-4a17-a253-dfa9d55afa27",
   "metadata": {},
   "source": [
    "## Task-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76d7ea89-c439-43f5-9afd-6b3a5d6146b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Ensure stopwords are available\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62564e18-4962-450c-a59a-950004d5d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload stopwords and compile regex once for efficiency\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "CLEAN_PATTERN = re.compile(r'[^a-z\\s]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6474f8a7-46e4-48b2-bc40-def0b26c84a8",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e65b9f04-34d9-4140-b29f-530c081010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean a single text string by:\n",
    "      - converting to lowercase,\n",
    "      - removing punctuation and digits,\n",
    "      - tokenizing and removing stopwords.\n",
    "    Returns the cleaned text.\n",
    "    \"\"\"\n",
    "    text = text.lower()                              # Convert to lowercase\n",
    "    text = CLEAN_PATTERN.sub('', text)               # Remove digits/punctuation\n",
    "    tokens = text.split()                            # Tokenize by whitespace\n",
    "    tokens = [word for word in tokens if word not in STOP_WORDS]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_reviews(df, text_column='review'):\n",
    "    \"\"\"\n",
    "    Preprocess all reviews in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "      df          : Pandas DataFrame containing reviews.\n",
    "      text_column : Column name containing raw text.\n",
    "      \n",
    "    Returns:\n",
    "      df with an added 'clean_review' column.\n",
    "    \"\"\"\n",
    "    df['clean_review'] = df[text_column].apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c870e8-e9fb-4682-a7b3-0cb6d3b338d7",
   "metadata": {},
   "source": [
    "### Feature Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f58c86b2-d449-4d8e-bfb9-c9658b974a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(texts, max_features=5000):\n",
    "    \"\"\"\n",
    "    Convert text data into numerical features using TF-IDF.\n",
    "    \n",
    "    Parameters:\n",
    "      texts        : Iterable of cleaned text strings.\n",
    "      max_features : Maximum number of features to consider.\n",
    "      \n",
    "    Returns:\n",
    "      vectorizer : Fitted TfidfVectorizer.\n",
    "      features   : Transformed text data as a sparse matrix.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    features = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c4d1b-70bf-40c9-a1f9-961df1a6d1a4",
   "metadata": {},
   "source": [
    "### Categorization (Model Training) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f210c36-4db1-4c41-845f-8824d89be1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X, y, model_type='logistic', max_iter=1000):\n",
    "    \"\"\"\n",
    "    Train a classifier to categorize reviews into positive or negative sentiment.\n",
    "    \n",
    "    Parameters:\n",
    "      X          : Feature matrix.\n",
    "      y          : Target labels.\n",
    "      model_type : Type of model to use ('logistic' currently supported).\n",
    "      max_iter   : Maximum iterations for model training.\n",
    "      \n",
    "    Returns:\n",
    "      A trained model.\n",
    "    \"\"\"\n",
    "    if model_type == 'logistic':\n",
    "        model = LogisticRegression(max_iter=max_iter)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Try 'logistic'.\")\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def evaluate_classifier(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the trained classifier on test data.\n",
    "    Prints a classification report along with accuracy, precision, and recall.\n",
    "    \n",
    "    Parameters:\n",
    "      model   : Trained classifier.\n",
    "      X_test  : Test features.\n",
    "      y_test  : Test labels.\n",
    "      \n",
    "    Returns:\n",
    "      Predictions made on X_test.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a015be91-faaf-4b48-b324-d59bbc7fb81f",
   "metadata": {},
   "source": [
    "### Pipeline Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1798ec5-3e39-42dc-9e36-0800585058bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(data_path, text_column='review', label_column='sentiment'):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline that:\n",
    "      - Loads the dataset.\n",
    "      - Preprocesses text data.\n",
    "      - Extracts features using TF-IDF.\n",
    "      - Splits data into training and test sets.\n",
    "      - Trains a classifier.\n",
    "      - Evaluates the classifier.\n",
    "    \n",
    "    Parameters:\n",
    "      data_path   : Path to the CSV dataset.\n",
    "      text_column : Name of the column with reviews.\n",
    "      label_column: Name of the column with sentiment labels.\n",
    "    \n",
    "    Returns:\n",
    "      model      : Trained classification model.\n",
    "      vectorizer : Fitted TF-IDF vectorizer.\n",
    "      df         : Processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Convert sentiment labels to binary values (assuming the dataset uses 'positive' and 'negative')\n",
    "    df[label_column] = df[label_column].map({'positive': 1, 'negative': 0})\n",
    "    \n",
    "    # Preprocess the reviews\n",
    "    df = preprocess_reviews(df, text_column=text_column)\n",
    "    \n",
    "    # Extract features from the cleaned reviews\n",
    "    vectorizer, features = extract_features(df['clean_review'])\n",
    "    \n",
    "    # Split data into training and testing sets (80/20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df[label_column], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the classifier\n",
    "    model = train_classifier(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    evaluate_classifier(model, X_test, y_test)\n",
    "    \n",
    "    return model, vectorizer, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ac9c7-541a-4757-ace5-5579f4a08b63",
   "metadata": {},
   "source": [
    "### Manual Validation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1edfaf-be07-4989-9e9a-cfbca6f3ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_manual_sample(model, vectorizer):\n",
    "    \"\"\"\n",
    "    Validate the classifier with a small, manually curated sample of reviews.\n",
    "    \n",
    "    Parameters:\n",
    "      model      : Trained classifier.\n",
    "      vectorizer : Fitted TF-IDF vectorizer.\n",
    "    \n",
    "    Prints the original review and its predicted sentiment.\n",
    "    \"\"\"\n",
    "    # Manually curated sample reviews\n",
    "    sample_reviews = [\n",
    "        \"I absolutely loved this movie, it was fantastic!\",\n",
    "        \"This product is terrible, I will never buy it again.\",\n",
    "        \"What a wonderful book, it warmed my heart.\",\n",
    "        \"Awful service, I'm very disappointed.\"\n",
    "    ]\n",
    "    \n",
    "    # Preprocess the sample reviews\n",
    "    cleaned_samples = [preprocess_text(review) for review in sample_reviews]\n",
    "    \n",
    "    # Convert the cleaned text to features using the already fitted vectorizer\n",
    "    sample_features = vectorizer.transform(cleaned_samples)\n",
    "    \n",
    "    # Predict sentiment for the sample reviews\n",
    "    predictions = model.predict(sample_features)\n",
    "    \n",
    "    # Map numeric predictions back to sentiment labels\n",
    "    sentiment_mapping = {1: \"positive\", 0: \"negative\"}\n",
    "    \n",
    "    print(\"\\nManual Validation on Curated Samples:\")\n",
    "    for review, pred in zip(sample_reviews, predictions):\n",
    "        print(\"Review: \", review)\n",
    "        print(\"Predicted Sentiment: \", sentiment_mapping[pred])\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a5e2a-2d48-4d82-bf5c-65a80edabc83",
   "metadata": {},
   "source": [
    "### Main Function to Run the Entire Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a10d238f-9e5c-4bc1-b8b7-80f9e0fa31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88      4961\n",
      "           1       0.88      0.90      0.89      5039\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy: 0.8872\n",
      "Precision: 0.8778743961352657\n",
      "Recall: 0.901567771383211\n",
      "\n",
      "Manual Validation on Curated Samples:\n",
      "Review:  I absolutely loved this movie, it was fantastic!\n",
      "Predicted Sentiment:  positive\n",
      "--------------------------------------------------\n",
      "Review:  This product is terrible, I will never buy it again.\n",
      "Predicted Sentiment:  negative\n",
      "--------------------------------------------------\n",
      "Review:  An average experience, nothing particularly memorable.\n",
      "Predicted Sentiment:  negative\n",
      "--------------------------------------------------\n",
      "Review:  What a wonderful book, it warmed my heart.\n",
      "Predicted Sentiment:  positive\n",
      "--------------------------------------------------\n",
      "Review:  Awful service, I'm very disappointed.\n",
      "Predicted Sentiment:  negative\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # File path to the dataset (update as needed)\n",
    "    data_path = 'C:/Users/asus/Desktop/Text_Extractor/imdb_reviews.csv'\n",
    "    \n",
    "    # Run the complete pipeline: load data, preprocess, extract features, train, and evaluate\n",
    "    model, vectorizer, df = run_pipeline(data_path)\n",
    "    \n",
    "    # Validate the model with a manually curated sample\n",
    "    validate_manual_sample(model, vectorizer)\n",
    "    \n",
    "    \n",
    "\n",
    "# Run the main function if this script is executed\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a493a-d70c-4d3c-adcb-d0617e83258a",
   "metadata": {},
   "source": [
    "### Insights and Challenges faced during the task:\n",
    "    Insights and Challenges Faced\n",
    "    1. Preprocessing needed careful balancing: Removing too much can lead to loss of sentiment cues.\n",
    "    2. The TF-IDF vectorization process is computationally intensive on large datasets; limiting features is crucial.\n",
    "    3. Tuning model hyperparameters is necessary for better performance.\n",
    "    4. Manual validation is important to ensure that the model generalizes well to unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
